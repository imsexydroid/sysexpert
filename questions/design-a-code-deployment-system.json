{
    "uid": "design-a-code-deployment-system",
    "name": "Design A Code-Deployment System",
    "acl": {
        "isFree": true,
        "isFreeForStudents": false,
        "productRequired": [
            "systemsexpert"
        ],
        "isAvailable": true
    },
    "releaseDate": "2020-02-15T12:00:00-05:00",
    "isReleased": true,
    "video": {
        "vimeoId": "391631170",
        "duration": 0,
        "annotations": [],
        "instructor": "",
        "style": "interview"
    },
    "prompt": "<p>Design a global and fast code-deployment system.</p>\n",
    "walkthrough": [
        {
            "title": "Gathering System Requirements",
            "content": "<p>\n  As with any systems design interview question, the first thing that we want to\n  do is to gather system requirements; we need to figure out what system we're\n  building exactly.\n</p>\n<p>\n  From the answers we were given to our clarifying questions (see Prompt Box),\n  we're building a system that involves repeatedly (in the order of thousands of\n  times per day) building and deploying code to <b>hundreds of thousands</b> of\n  machines spread out across <b>5-10 regions</b> around the world.\n</p>\n<p>\n  Building code will involve grabbing snapshots of source code using commit SHA\n  identifiers; beyond that, we can assume that the actual implementation details\n  of the building action are taken care of. In other words, we don't need to\n  worry about how we would build JavaScript code or C++ code; we just need to\n  design the system that enables the repeated building of code.\n</p>\n<p>\n  Building code will take up to <b>15 minutes</b>, it'll result in a binary file of up\n  to <b>10GB</b>, and we want to have the entire deployment process (building and\n  deploying code to our target machines) take at most <b>30 minutes</b>.\n</p>\n<p>\n  Each build will need a clear end-state (<b>SUCCESS</b> or <b>FAILURE</b>), and though we\n  care about availability (2 to 3 nines), we don't need to optimize too much on\n  this dimension.\n</p>"
        },
        {
            "title": "Coming Up With A Plan",
            "content": "<p>\n  It's important to organize ourselves and to lay out a clear plan regarding how\n  we're going to tackle our design. What are the major, distinguishable\n  components of our how system?\n</p>\n<p>\n  It seems like this system can actually very simply be divided into two clear\n  subsystems:\n</p>\n<ul>\n  <li>the Build System that builds code into binaries</li>\n  <li>\n    the Deployment System that deploys binaries to our machines across the\n    world\n  </li>\n</ul>\n<p>\n  Note that these subsystems will of course have many components themselves, but\n  this is a very straightforward initial way to approach our problem.\n</p>"
        },
        {
            "title": "Build System -- General Overview",
            "content": "<p>\n  From a high-level perspective, we can call the process of building code into a\n  binary a <b>job</b>, and we can design our build system as a queue of jobs.\n  Jobs get added to the queue, and each job has a commit identifier (the commit\n  SHA) for what version of the code it should build and the name of the artifact\n  that will be created (the name of the resulting binary). Since we're agnostic\n  to the type of the code being built, we can assume that all languages are\n  handled automatically here.\n</p>\n<p>\n  We can have a pool of servers (workers) that are going to handle all of these\n  jobs. Each worker will repeatedly take jobs off the queue (in a\n  <b>FIFO manner</b>\u2014no prioritization for now), build the relevant binaries\n  (again, we're assuming that the actual implementation details of building code are\n  given to us), and write the resulting binaries to blob storage (<b>Google\n  Cloud Storage</b> or <b>S3</b> for instance). Blob storage makes\n  sense here, because binaries are literally blobs of data.\n</p>"
        },
        {
            "title": "Build System -- Job Queue",
            "content": "<p>\n  A naive design of the job queue would have us implement it in memory (just as\n  we would implement a queue in coding interviews), but this implementation is\n  very problematic; if there's a failure in our servers that hold this queue, we\n  lose the entire state of our jobs: queued jobs and past jobs.\n</p>\n<p>\n  It seems like we would be unnecessarily complicating matters by trying to\n  optimize around this in-memory type of storage, so we're likely better off\n  implementing the queue using a SQL database.\n</p>"
        },
        {
            "title": "Build System -- SQL Job Queue",
            "content": "<p>\n  We can have a <b>jobs</b> table in our SQL database where every record in the\n  database represents a job, and we can use record-creation timestamps as the\n  queue's ordering mechanism.\n</p>\n<p>\n  Our table will be:\n</p>\n<ul>\n  <li>\n    id: <i>string</i>, the ID of the job, auto-generated\n  </li>\n  <li>\n    created_at: <i>timestamp</i>\n  </li>\n  <li>\n    commit_sha: <i>string</i>\n  </li>\n  <li>\n      name: <i>string</i>, the pointer to the job's eventual binary in blob storage\n  </li>\n  <li>\n    status: <i>string</i>, <b>QUEUED</b>, <b>RUNNING</b>, <b>SUCCEEDED</b>, <b>FAILED</b>\n  </li>\n</ul>\n<p>\n  We can implement the actual dequeuing mechanism by looking at the oldest\n  creation_timestamp with a QUEUED status. This means that we'll likely want to\n  index our table on both created_at and status.\n</p>"
        },
        {
            "title": "Build System -- Concurrency",
            "content": "<p>\n  <b>ACID transactions</b> will make it safe for potentially hundreds of workers\n  to grab jobs off the queue without unintentionally running the same job twice\n  (we'll avoid race conditions). Our actual transaction will look like this:\n</p>\n\n<pre>\n  BEGIN TRANSACTION;\n  SELECT * FROM jobs_table WHERE status = 'QUEUED' ORDER BY created_at ASC LIMIT 1;\n  // if there's none, we ROLLBACK;\n  UPDATE jobs_table SET status = 'RUNNING' WHERE id = id from previous query;\n  COMMIT;\n</pre>\n<p>\n  All of the workers will be running this transaction every so often to dequeue\n  the next job; let's say every 5 seconds. If we arbitrarily assume that we'll\n  have 100 workers sharing the same queue, we'll have 100/5 = 20 reads per\n  second, which is very easy to handle for a SQL database.\n</p>"
        },
        {
            "title": "Build System -- Lost Jobs",
            "content": "<p>\n  Since we're designing a large-scale system, we have to expect and handle edge\n  cases. Here, what if there's a network partition with our workers or one of\n  our workers dies mid-build? Since builds last around 15 minutes on average,\n  this will very likely happen. In this case, we want to avoid having a \"lost\n  job\" that we were never made aware of, and with our current design, the job\n  will remain RUNNING forever. How do we handle this?\n</p>\n<p>\n  We could have an extra column on our <b>jobs</b> table called last_heartbeat.\n  This will be updated in a heartbeat fashion by the worker running a particular\n  job, where that worker will update the relevant row in the table every 3-5\n  minutes to just let us know that it's still running the job.\n</p>\n<p>\n  We can then have a completely separate service that polls the table every so\n  often (say, every 5 minutes, depending on how responsive we want this build\n  system to be), checks all of the <b>RUNNING</b> jobs, and if their last_heartbeat was\n  last modified longer than 2 heartbeats ago (we need some margin of error\n  here), then something's likely wrong, and this service can reset the status of\n  the relevant jobs to <b>QUEUED</b>, which would effectively bring them back to the\n  front of the queue.\n</p>\n<p>\n  The transaction that this auxiliary service will perform will look something\n  like this:\n</p>\n<pre>\n  UPDATE jobs_table SET status = 'QUEUED' WHERE\n    status = 'RUNNING' AND\n    last_heartbeat < NOW() - 10 minutes;\n</pre>"
        },
        {
            "title": "Build System -- Scale Estimation",
            "content": "<p>\n  We previously arbitrarily assumed that we would have 100 workers, which made\n  our SQL-database queue able to handle the expected load. We should try to\n  estimate if this number of workers is actually realistic.\n</p>\n<p>\n  With some back-of-the-envelope math, we can see that, since a build can take\n  up to 15 minutes, a single worker can run 4 jobs per hour, or ~100 (96) jobs\n  per day. Given thousands of builds per day (say, 5000-10000), this means that\n  we would need <b>50-100 workers</b> (5000 / 100). So our arbitrary figure was\n  accurate.\n</p>\n<p>\n  Even if the builds aren't uniformly spread out (in other words, they peak\n  during work hours), our system scales horizontally very easily. We can\n  automatically add or remove workers whenever the load warrants it. We can also\n  scale our system vertically by making our workers more powerful, thereby\n  reducing the build time.\n</p>"
        },
        {
            "title": "Build System -- Storage",
            "content": "\n<p>\n  We previously mentioned that we would store binaries in blob storage\n  (<b>GCS</b>). Where does this storage fit into our queueing system exactly?\n</p>\n<p>\n  When a worker completes a build, it can store the binary in GCS\n  <i>before</i> updating the relevant row in the <b>jobs</b> table. This will\n  ensure that a binary has been persisted before its relevant job is marked as\n  <b>SUCCEEDED</b>.\n</p>\n<p>\n  Since we're going to be deploying our binaries to machines spread across the\n  world, it'll likely make sense to have regional storage rather than just a\n  single global blob store.\n</p>\n<p>\n  We can design our system based on regional clusters around the world (in our\n  5-10 global regions). Each region can have a blob store (a regional GCS\n  bucket). Once a worker successfully stores a binary in our main blob store,\n  the worker is released and can run another job, while the main blob store\n  performs some asynchronous replication to store the binary in all of the\n  regional GCS buckets. Given 5-10 regions and 10GB files, this step should take\n  no more than 5-10 minutes, bringing our total build-and-deploy duration so far\n  to roughly 20-25 minutes (15 minutes for a build and 5-10 minutes for global\n  replication of the binary).\n</p>\n<p></p>"
        },
        {
            "title": "Deployment System -- General Overview",
            "content": "<p>\n  From a high-level perspective, our actual deployment system will need to allow\n  for the very fast distribution of 10GB binaries to hundreds of thousands of\n  machines across all of our global regions. We're likely going to want some\n  service that tells us when a binary has been replicated in all regions,\n  another service that can serve as the source of truth for what binary should\n  currently be run on all machines, and finally a peer-to-peer-network design\n  for our actual machines across the world.\n</p>"
        },
        {
            "title": "Deployment System -- Replication-Status Service",
            "content": "<p>\n  We can have a global service that continuously checks all regional GCS buckets\n  and aggregates the replication status for successful builds (in other words,\n  checks that a given binary in the main blob store has been replicated across\n  all regions). Once a binary has been replicated across all regions, this\n  service updates a separate SQL database with rows containing the name of a\n  binary and a <b>replication_status</b>. Once a binary has a \"complete\"\n  <b>replication_status</b>, it's officially deployable.\n</p>"
        },
        {
            "title": "Deployment System -- Blob Distribution",
            "content": "\n<p>\n  Since we're going to deploy 10 GBs to hundreds of thousands of machines, even\n  with our regional clusters, having each machine download a 10GB file one after\n  the other from a regional blob store is going to be extremely slow. A\n  peer-to-peer-network approach will be much faster and will allow us to hit our\n  30-minute time frame for deployments. All of our regional clusters will behave\n  as peer-to-peer networks.\n</p>"
        },
        {
            "title": "Deployment System -- Trigger",
            "content": "\n<p>\n  Let's describe what happens when an engineer presses a button on some internal\n  UI that says \"Deploy build/binary B1 to every machine globally\". This is the\n  action that triggers the binary downloads on all the regional peer-to-peer\n  networks.\n</p>\n<p>\n  To simplify this process and to support having multiple builds getting\n  deployed concurrently, we can design this in a goal-state oriented manner.\n</p>\n<p>\n  The goal-state will be the desired build version at any point in time and\n  will look something like: \"current_build: <b>B1</b>\", and this can be stored\n  in some dynamic configuration service (a <b>key-value store</b> like\n  <b>Etcd</b> or <b>ZooKeeper</b>). We'll have a global goal-state as well as\n  regional goal-states.\n</p>\n<p>\n  Each regional cluster will have a K-V store that holds configuration for that\n  cluster about what builds should be running on that cluster, and we'll also\n  have a global K-V store.\n</p>\n<p>\n  When an engineer clicks the \"Deploy build/binary B1\" button, our global K-V\n  store's build_version will get updated. Regional K-V stores will be\n  continuously polling the global K-V store (say, every 10 seconds) for updates\n  to the build_version and will update themselves accordingly.\n</p>\n<p>\n  Machines in the clusters/regions will be polling the relevant regional K-V\n  store, and when the build_version changes, they'll try to fetch that build\n  from the P2P network and run the binary.\n</p>"
        },
        {
            "title": "System Diagram",
            "content": "<img\n  width=\"100%\"\n  src=\"https://assets.algoexpert.io/course-assets/systemsexpert/code-deployment-system-diagram.svg\"\n  alt=\"Final Systems Architecture\"\n/>"
        }
    ],
    "hints": [
        {
            "question": "What exactly do we mean by a code-deployment system? Are we talking about building, testing, and shipping code?",
            "answer": "We want to design a system that takes code, builds it into a binary (an opaque blob of data\u2014the compiled code), and deploys the result globally in an efficient and scalable way. We don't need to worry about testing code; let's assume that's already covered."
        },
        {
            "question": "What part of the software-development lifecycle, so to speak, are  we designing this for? Is this process of building and deploying code happening when code is being submitted for code review, when code is being merged into a codebase, or when code is being shipped?",
            "answer": "Once code is merged into the trunk or master branch of a central code repository, engineers should be able to trigger a build and deploy that build (through a UI, which we're not designing). At that point, the code has already been reviewed and is ready to ship. So to clarify, we're not designing the system that handles code being submitted for review or being merged into a master branch\u2014just the system that takes merged code, builds it, and deploys it."
        },
        {
            "question": "Are we essentially trying to ship code to production by sending it to, presumably, all of our application servers around the world?",
            "answer": "Yes, exactly."
        },
        {
            "question": "How many machines are we deploying to? Are they located all over the world?",
            "answer": "We want this system to scale massively to hundreds of thousands of machines spread across 5-10 regions throughout the world."
        },
        {
            "question": "This sounds like an internal system. Is there any sense of urgency in deploying this code? Can we afford failures in the deployment process? How fast do we want a single deployment to take?",
            "answer": "This is an internal system, but we'll want to have decent availability, because many outages are resolved by rolling forward or rolling back buggy code, so this part of the infrastructure may be necessary to avoid certain terrible situations. In terms of failure tolerance, any build should eventually reach a SUCCESS or FAILURE state. Once a binary has been successfully built, it should be shippable to all machines globally within 30 minutes."
        },
        {
            "question": "So it sounds like we want our system to be available, but not necessarily highly available, we want a clear end-state for builds, and we want the entire process of building and deploying code to take roughly 30 minutes. Is that correct?",
            "answer": "Yes, that's correct."
        },
        {
            "question": "How often will we be building and deploying code, how long does it take to build code, and how big can the binaries that we'll be deploying get?",
            "answer": "Engineering teams deploy hundreds of services or web applications, thousands of times per day; building code can take up to 15 minutes; and the final binaries can reach sizes of up to 10 GB. The fact that we might be dealing with hundreds of different applications shouldn't matter though; you're just designing the build pipeline and deployment system, which are agnostic to the types of applications that are getting deployed."
        },
        {
            "question": "When building code, how do we have access to the actual code? Is there some sort of reference that we can use to grab code to build?",
            "answer": "Yes; you can assume that you'll be building code from commits that have been merged into a master branch. These commits have SHA identifiers (effectively arbitrary strings) that you can use to download the code that needs to be built."
        }
    ]
}